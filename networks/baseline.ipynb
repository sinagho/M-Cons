{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\DL1402\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.nn import functional as F\n",
    "from timm.models.layers import DropPath, to_2tuple\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DWConvLKA(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super(DWConvLKA, self).__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dwconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0., linear=False):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Conv2d(in_features, hidden_features, 1)\n",
    "        self.dwconv = DWConvLKA(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Conv2d(hidden_features, out_features, 1)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.linear = linear\n",
    "        if self.linear:\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        if self.linear:\n",
    "            x = self.relu(x)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(dim, dim, 5, padding=2, groups=dim)\n",
    "        self.conv_spatial = nn.Conv2d(\n",
    "            dim, dim, 7, stride=1, padding=9, groups=dim, dilation=3)\n",
    "        self.conv1 = nn.Conv2d(dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.clone()\n",
    "        attn = self.conv0(x)\n",
    "        attn = self.conv_spatial(attn)\n",
    "        attn = self.conv1(attn)\n",
    "        return u * attn\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.proj_1 = nn.Conv2d(d_model, d_model, 1)\n",
    "        self.activation = nn.GELU()\n",
    "        self.spatial_gating_unit = AttentionModule(d_model)\n",
    "        self.proj_2 = nn.Conv2d(d_model, d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shorcut = x.clone()\n",
    "        x = self.proj_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.spatial_gating_unit(x)\n",
    "        x = self.proj_2(x)\n",
    "        x = x + shorcut\n",
    "        return x\n",
    "\n",
    "\n",
    "class LKABlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 mlp_ratio=4.,\n",
    "                 drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 linear=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)  # build_norm_layer(norm_cfg, dim)[1]\n",
    "        self.attn = SpatialAttention(dim)\n",
    "        self.drop_path = DropPath(\n",
    "            drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim)  # build_norm_layer(norm_cfg, dim)[1]\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=act_layer, drop=drop, linear=linear)\n",
    "        layer_scale_init_value = 1e-2\n",
    "        self.layer_scale_1 = nn.Parameter(\n",
    "            layer_scale_init_value * torch.ones((dim)), requires_grad=True)\n",
    "        self.layer_scale_2 = nn.Parameter(\n",
    "            layer_scale_init_value * torch.ones((dim)), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B, N, C = x.shape\n",
    "        # x = x.permute(0, 2, 1).view(B, C, H, W)\n",
    "        y = x.permute(0, 2, 3, 1)  # b h w c, because norm requires this\n",
    "        y = self.norm1(y)\n",
    "        y = y.permute(0, 3, 1, 2)  # b c h w, because attn requieres this\n",
    "        y = self.attn(y)\n",
    "        y = self.layer_scale_1.unsqueeze(-1).unsqueeze(-1) * y\n",
    "        y = self.drop_path(y)\n",
    "        x = x + y\n",
    "        # x = x + self.drop_path(self.layer_scale_1.unsqueeze(-1).unsqueeze(-1)\n",
    "        #                       * self.attn(self.norm1(x)))\n",
    "\n",
    "        y = x.permute(0, 2, 3, 1)  # b h w c, because norm requires this\n",
    "        y = self.norm2(y)\n",
    "        y = y.permute(0, 3, 1, 2)  # b c h w, because attn requieres this\n",
    "        y = self.mlp(y)\n",
    "        y = self.layer_scale_2.unsqueeze(-1).unsqueeze(-1) * y\n",
    "        y = self.drop_path(y)\n",
    "        x = x + y\n",
    "        # x = x + self.drop_path(self.layer_scale_2.unsqueeze(-1).unsqueeze(-1)\n",
    "        #                       * self.mlp(self.norm2(x)))\n",
    "        # x = x.view(B, C, N).permute(0, 2, 1)\n",
    "        # print(\"LKA return shape: {}\".format(x.shape))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExpand(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, dim_scale=2, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.expand = nn.Linear(dim, 2 * dim, bias=False) if dim_scale == 2 else nn.Identity()\n",
    "        self.norm = norm_layer(dim // dim_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        # print(\"x_shape-----\",x.shape)\n",
    "        H, W = self.input_resolution\n",
    "        x = self.expand(x)\n",
    "\n",
    "        B, L, C = x.shape\n",
    "        # print(x.shape)\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x = rearrange(x, \"b h w (p1 p2 c)-> b (h p1) (w p2) c\", p1=2, p2=2, c=C // 4)\n",
    "        x = x.view(B, -1, C // 4)\n",
    "        x = self.norm(x.clone())\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FinalPatchExpand_X4(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, dim_scale=4, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.dim_scale = dim_scale\n",
    "        self.expand = nn.Linear(dim, 16 * dim, bias=False)\n",
    "        self.output_dim = dim\n",
    "        self.norm = norm_layer(self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        x = self.expand(x)\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x = rearrange(\n",
    "            x, \"b h w (p1 p2 c)-> b (h p1) (w p2) c\", p1=self.dim_scale, p2=self.dim_scale, c=C // (self.dim_scale ** 2)\n",
    "        )\n",
    "        x = x.view(B, -1, self.output_dim)\n",
    "        x = self.norm(x.clone())\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptGenBlock(nn.Module):\n",
    "    def __init__(self,prompt_dim=48,prompt_len=5,prompt_size = 96,lin_dim = 192):\n",
    "        super(PromptGenBlock,self).__init__()\n",
    "        self.prompt_param = nn.Parameter(torch.rand(1,prompt_len,prompt_dim,prompt_size,prompt_size))\n",
    "        self.linear_layer = nn.Linear(lin_dim,prompt_len)\n",
    "        self.conv3x3 = nn.Conv2d(prompt_dim,prompt_dim,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        B,C,H,W = x.shape\n",
    "        emb = x.mean(dim=(-2,-1))\n",
    "        prompt_weights = F.softmax(self.linear_layer(emb),dim=1)\n",
    "        prompt = prompt_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * self.prompt_param.unsqueeze(0).repeat(B,1,1,1,1,1).squeeze(1)\n",
    "        prompt = torch.sum(prompt,dim=1)\n",
    "        prompt = F.interpolate(prompt,(H,W),mode=\"bilinear\")\n",
    "        prompt = self.conv3x3(prompt)\n",
    "        \n",
    "\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1,96,56,56).mean(dim=(-2,-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptGenBlockSina(nn.Module):\n",
    "    def __init__(self,prompt_dim=48,prompt_len=5,prompt_size = 96,lin_dim = 192):\n",
    "        super(PromptGenBlockSina,self).__init__()\n",
    "\n",
    "        self.prompt_param = nn.Parameter(torch.rand(1,prompt_len,prompt_dim,prompt_size,prompt_size))\n",
    "        self.linear_layer = nn.Linear(lin_dim,prompt_len)\n",
    "\n",
    "        self.conv3x3 = nn.Conv2d(prompt_dim,prompt_dim,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        B,C,H,W = x.shape\n",
    "        emb = x.mean(dim=(-2,-1)) # B, C\n",
    "        prompt_weights = F.softmax(self.linear_layer(emb),dim=1) # B, C\n",
    "        \n",
    "        prompt_ = prompt_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        print(prompt_.shape)\n",
    "        \n",
    "        prompt = prompt_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * self.prompt_param.unsqueeze(0).repeat(B,1,1,1,1,1).squeeze(1)\n",
    "\n",
    "        prompt__ = self.prompt_param.unsqueeze(0).repeat(B,1,1,1,1,1).squeeze(1)\n",
    "        print(prompt__.shape)\n",
    "        \n",
    "        prompt = torch.sum(prompt,dim=1)\n",
    "        print(prompt.shape)\n",
    "        prompt = F.interpolate(prompt,(H,W),mode=\"bilinear\")\n",
    "        prompt = self.conv3x3(prompt)\n",
    "        \n",
    "\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_ratio = [32, 16, 8, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptGenBlockSinaV1(nn.Module):\n",
    "    def __init__(self,prompt_dim=48,prompt_ratio=4,input_size = 96,lin_dim = 192):\n",
    "        super(PromptGenBlockSinaV1,self).__init__()\n",
    "\n",
    "        prompt_len = prompt_ratio * prompt_dim \n",
    "\n",
    "        self.prompt_param = nn.Parameter(torch.rand(1,prompt_len, input_size, input_size))\n",
    "\n",
    "        self.linear_layer = nn.Linear(lin_dim,prompt_len)\n",
    "\n",
    "        self.conv3x3 = nn.Conv2d(prompt_len,prompt_dim,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        B,C,H,W = x.shape\n",
    "\n",
    "        emb = x.mean(dim=(-2,-1)) # B, C\n",
    "\n",
    "        \n",
    "        prompt_weights = F.softmax(self.linear_layer(emb),dim=1) # B, C\n",
    "\n",
    "        \n",
    "        prompt_weights = prompt_weights.unsqueeze(-1).unsqueeze(-1).expand_as(self.prompt_param)\n",
    "\n",
    "        prompt = prompt_weights * self.prompt_param\n",
    "\n",
    "        prompt = self.conv3x3(prompt)\n",
    "\n",
    "        print(prompt.shape)\n",
    "        \n",
    "\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 768, 7, 7]), 23.74656)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PromptGenBlockSinaV1(prompt_dim=768,\n",
    "                       prompt_ratio= 4,\n",
    "                       input_size=7,\n",
    "                       lin_dim=768)\n",
    "\n",
    "model(torch.randn(1,768, 7, 7)).shape, calculate_params_in_millions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PromptGenBlockSina(prompt_dim=96,\n",
    "                       prompt_len= 5,\n",
    "                       input_size=96,\n",
    "                       lin_dim=96)\n",
    "\n",
    "model(torch.randn(1,96, 7, 7)).shape, calculate_params_in_millions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_st(nn.Module):\n",
    "    def __init__(self, dim, num_heads, bias):\n",
    "        super(Attention_st, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        self.qkv = nn.Conv2d(dim, dim*3, kernel_size=1, bias=bias)\n",
    "        self.qkv_dwconv = nn.Conv2d(dim*3, dim*3, kernel_size=3, stride=1, padding=1, groups=dim*3, bias=bias)\n",
    "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b,c,h,w = x.shape\n",
    "\n",
    "        qkv = self.qkv_dwconv(self.qkv(x))\n",
    "        q,k,v = qkv.chunk(3, dim=1)   \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "\n",
    "        q = torch.nn.functional.normalize(q, dim=-1)\n",
    "        k = torch.nn.functional.normalize(k, dim=-1)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        out = (attn @ v)\n",
    "        \n",
    "        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
    "\n",
    "        out = self.project_out(out)\n",
    "        return out\n",
    "\n",
    "## Multi-DConv Head Transposed Self-Attention (MDTA)\n",
    "class Attention_st_cross(nn.Module):\n",
    "    def __init__(self, dim, num_heads, bias):\n",
    "        dim = dim //2\n",
    "        super(Attention_st_cross, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        self.kv = nn.Conv2d(dim, 2*dim*2, kernel_size=1, bias=bias)\n",
    "        self.q = nn.Conv2d(dim, 2*dim, kernel_size=1, bias=bias)\n",
    "        self.kv_dwconv = nn.Conv2d(2*dim*2, 2*dim*2, kernel_size=3, stride=1, padding=1, groups=dim*2, bias=bias)\n",
    "        self.project_out = nn.Conv2d(2*dim, 2*dim, kernel_size=1, bias=bias)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        b,c,h,w = x.shape\n",
    "\n",
    "        kv = self.kv_dwconv(self.kv(x[:, :c//2]))\n",
    "        k,v = kv.chunk(2, dim=1)   \n",
    "        q = self.q(x[:, :c//2])\n",
    "\n",
    "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "\n",
    "        q = torch.nn.functional.normalize(q, dim=-1)\n",
    "        k = torch.nn.functional.normalize(k, dim=-1)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        out = (attn @ v)\n",
    "        \n",
    "        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
    "\n",
    "        out = self.project_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "\n",
    "class WithBias_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(WithBias_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(-1, keepdim=True)\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_3d(x):\n",
    "    return rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "def to_4d(x,h,w):\n",
    "    return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)\n",
    "\n",
    "class LayerNormst(nn.Module):\n",
    "    def __init__(self, dim, LayerNorm_type):\n",
    "        super(LayerNormst, self).__init__()\n",
    "        if LayerNorm_type =='BiasFree':\n",
    "            self.body = BiasFree_LayerNorm(dim)\n",
    "        else:\n",
    "            self.body = WithBias_LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        return to_4d(self.body(to_3d(x)), h, w)\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_expansion_factor, bias):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        hidden_features = int(dim*ffn_expansion_factor)\n",
    "\n",
    "        self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n",
    "\n",
    "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.project_in(x)\n",
    "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
    "        x = F.gelu(x1) * x2\n",
    "        x = self.project_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, ffn_expansion_factor, bias, LayerNorm_type, cross= False):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.norm1 = LayerNormst(dim, LayerNorm_type)\n",
    "        if not cross:\n",
    "            self.attn = Attention_st(dim, num_heads, bias)\n",
    "        else:\n",
    "            self.attn = Attention_st_cross(dim, num_heads, bias)\n",
    "        self.norm2 = LayerNormst(dim, LayerNorm_type)\n",
    "        self.ffn = FeedForward(dim, ffn_expansion_factor, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecoderLayerLKA(nn.Module):\n",
    "    def __init__(\n",
    "            self, input_size: tuple, in_out_chan: tuple, n_class=9,\n",
    "            norm_layer=nn.LayerNorm, is_last=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_dim = in_out_chan[0]\n",
    "        x1_dim = in_out_chan[1]\n",
    "        \n",
    "        if not is_last:\n",
    "            self.x1_linear = nn.Linear(x1_dim, out_dim)\n",
    "            #self.ag_attn = MultiScaleGatedAttn(dim=x1_dim)\n",
    "            self.ag_attn_norm = nn.LayerNorm(out_dim)\n",
    "\n",
    "            self.layer_up = PatchExpand(input_resolution=input_size, dim=out_dim, dim_scale=2, norm_layer=norm_layer)\n",
    "            self.last_layer = None\n",
    "        else:\n",
    "            self.x1_linear = nn.Linear(x1_dim, out_dim)\n",
    "            #self.ag_attn = MultiScaleGatedAttn(dim=x1_dim)\n",
    "            self.ag_attn_norm = nn.LayerNorm(out_dim)\n",
    "\n",
    "            self.layer_up = FinalPatchExpand_X4(\n",
    "                input_resolution=input_size, dim=out_dim, dim_scale=4, norm_layer=norm_layer\n",
    "            )\n",
    "            self.last_layer = nn.Conv2d(out_dim, n_class, 1)\n",
    "\n",
    "        \n",
    "        self.layer_lka_1 = LKABlock(dim=out_dim)\n",
    "        ## Prompt Module must be located here.\n",
    "\n",
    "        self.layer_lka_2 = LKABlock(dim=out_dim)\n",
    "\n",
    "        \n",
    "\n",
    "        def init_weights(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.LayerNorm):\n",
    "                    nn.init.ones_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.Conv2d):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "\n",
    "        init_weights(self)\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        if x2 is not None:  # skip connection exist\n",
    "            x2 = x2.contiguous()\n",
    "            # b, c, h, w = x1.shape\n",
    "            b2, h2, w2, c2 = x2.shape  # e.g: 1 28 28 320, 1 56 56 128\n",
    "            x2 = x2.view(b2, -1, c2)  # e.g: 1 784 320, 1 3136 128\n",
    "\n",
    "            x1_expand = self.x1_linear(x1)  # e.g: 1 784 256 --> 1 784 320, 1 3136 160 --> 1 3136 128\n",
    "\n",
    "            x2_new = x2.view(x2.size(0), x2.size(2), x2.size(1) // w2, x2.size(1) // h2) # B, C, H, W\n",
    "            \n",
    "\n",
    "            x1_expand = x1_expand.view(x2.size(0), x2.size(2), x2.size(1) // w2, x2.size(1) // h2) # B, C, H, W\n",
    "\n",
    "            # print(f'the x1_expand shape is: {x1_expand.shape}\\n\\t the x2_new shape is: {x2_new.shape}')\n",
    "\n",
    "            #attn_gate = self.ag_attn(x=x2_new, g=x1_expand)  # B C H W\n",
    "\n",
    "            cat_linear_x = x1_expand + x2_new  # B C H W\n",
    "            cat_linear_x = cat_linear_x.permute(0, 2, 3, 1)  # B H W C\n",
    "            cat_linear_x = self.ag_attn_norm(cat_linear_x)  # B H W C\n",
    "\n",
    "            cat_linear_x = cat_linear_x.permute(0, 3, 1, 2).contiguous()  # B C H W\n",
    "\n",
    "            tran_layer_1 = self.layer_lka_1(cat_linear_x)\n",
    "            # print(tran_layer_1.shape)\n",
    "            tran_layer_2 = self.layer_lka_2(tran_layer_1)\n",
    "\n",
    "            tran_layer_2 = tran_layer_2.view(tran_layer_2.size(0), tran_layer_2.size(3) * tran_layer_2.size(2),\n",
    "                                             tran_layer_2.size(1))\n",
    "            if self.last_layer:\n",
    "                out = self.last_layer(\n",
    "                    self.layer_up(tran_layer_2).view(b2, 4 * h2, 4 * w2, -1).permute(0, 3, 1, 2))  # 1 9 224 224\n",
    "            else:\n",
    "                out = self.layer_up(tran_layer_2)  # 1 3136 160\n",
    "        else:\n",
    "            out = self.layer_up(x1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_0 = torch.randn(1, 96, 56, 56).cuda() # skip 3\n",
    "input_1 = torch.randn(1, 192, 28, 28).cuda() # skip 2\n",
    "input_2 = torch.randn(1, 384, 14, 14).cuda() # skip 1\n",
    "input_3 = torch.randn(1, 768, 7, 7).cuda() # X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_3 = MyDecoderLayerLKA(input_size=(7,7),\n",
    "                               in_out_chan=([768, 768])).cuda()\n",
    "\n",
    "decoder_2 = MyDecoderLayerLKA(input_size=(14,14),\n",
    "                               in_out_chan=([384, 384])).cuda()\n",
    "decoder_1 = MyDecoderLayerLKA(input_size=(28,28),\n",
    "                               in_out_chan=([192, 192])).cuda()\n",
    "decoder_0 = MyDecoderLayerLKA(input_size=(56, 56), \n",
    "                              in_out_chan=([96, 96]),\n",
    "                              is_last=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, c, _, _ = input_3.shape\n",
    "\n",
    "output_3 = decoder_3(input_3.permute(0, 2, 3, 1).view(b,-1, c))\n",
    "output_2 = decoder_2(output_3, input_2.permute(0,2 , 3, 1))\n",
    "output_1 = decoder_1(output_2, input_1.permute(0, 2, 3, 1))\n",
    "output_0 = decoder_0(output_1, input_0.permute(0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the output 3 shape: torch.Size([1, 196, 384])\n",
      "the output 2 shape: torch.Size([1, 784, 192]) \n",
      "the output 1 shape: torch.Size([1, 3136, 96]) \n",
      "the output 0 shape: torch.Size([1, 9, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(f\"the output 3 shape: {output_3.shape}\\nthe output 2 shape: {output_2.shape} \\nthe output 1 shape: {output_1.shape} \\nthe output 0 shape: {output_0.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecoderLayerLKA_Prompt(nn.Module):\n",
    "    def __init__(\n",
    "            self, input_size: tuple, in_out_chan: tuple, n_class=9,\n",
    "            norm_layer=nn.LayerNorm, is_last=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_dim = in_out_chan[0]\n",
    "        x1_dim = in_out_chan[1]\n",
    "        \n",
    "        if not is_last:\n",
    "            self.x1_linear = nn.Linear(x1_dim, out_dim)\n",
    "            #self.ag_attn = MultiScaleGatedAttn(dim=x1_dim)\n",
    "            self.ag_attn_norm = nn.LayerNorm(out_dim)\n",
    "\n",
    "            self.layer_up = PatchExpand(input_resolution=input_size, dim=out_dim, dim_scale=2, norm_layer=norm_layer)\n",
    "            self.last_layer = None\n",
    "        else:\n",
    "            self.x1_linear = nn.Linear(x1_dim, out_dim)\n",
    "            #self.ag_attn = MultiScaleGatedAttn(dim=x1_dim)\n",
    "            self.ag_attn_norm = nn.LayerNorm(out_dim)\n",
    "\n",
    "            self.layer_up = FinalPatchExpand_X4(\n",
    "                input_resolution=input_size, dim=out_dim, dim_scale=4, norm_layer=norm_layer\n",
    "            )\n",
    "            self.last_layer = nn.Conv2d(out_dim, n_class, 1)\n",
    "\n",
    "        \n",
    "        self.layer_lka_1 = LKABlock(dim=out_dim)\n",
    "        ## Prompt Module must be located here.\n",
    "\n",
    "        #dim_p = int(out_dim * 0.75)\n",
    "        dim_p = out_dim\n",
    "        # self.prompt1 = PromptGenBlock(prompt_dim=dim_p,\n",
    "        #                               prompt_len=5,\n",
    "        #                               prompt_size = dim_p,\n",
    "        #                               lin_dim = dim_p)\n",
    "        \n",
    "        # self.noise_level1 = TransformerBlock(dim=int(dim_p*2**1) ,\n",
    "        #                                      num_heads=1, \n",
    "        #                                      ffn_expansion_factor=2.66, \n",
    "        #                                      bias=False, LayerNorm_type='WithBias')\n",
    "        \n",
    "        self.reduce_noise_level1 = nn.Conv2d(int(dim_p*2),int(dim_p*1),kernel_size=1,bias=False)\n",
    "\n",
    "\n",
    "\n",
    "        self.layer_lka_2 = LKABlock(dim=out_dim)\n",
    "\n",
    "        \n",
    "\n",
    "        def init_weights(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.LayerNorm):\n",
    "                    nn.init.ones_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.Conv2d):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "\n",
    "        init_weights(self)\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        if x2 is not None:  # skip connection exist\n",
    "            x2 = x2.contiguous()\n",
    "            # b, c, h, w = x1.shape\n",
    "            b2, h2, w2, c2 = x2.shape  # e.g: 1 28 28 320, 1 56 56 128\n",
    "            x2 = x2.view(b2, -1, c2)  # e.g: 1 784 320, 1 3136 128\n",
    "\n",
    "            x1_expand = self.x1_linear(x1)  # e.g: 1 784 256 --> 1 784 320, 1 3136 160 --> 1 3136 128\n",
    "\n",
    "            x2_new = x2.view(x2.size(0), x2.size(2), x2.size(1) // w2, x2.size(1) // h2) # B, C, H, W\n",
    "            \n",
    "\n",
    "            x1_expand = x1_expand.view(x2.size(0), x2.size(2), x2.size(1) // w2, x2.size(1) // h2) # B, C, H, W\n",
    "\n",
    "            # print(f'the x1_expand shape is: {x1_expand.shape}\\n\\t the x2_new shape is: {x2_new.shape}')\n",
    "\n",
    "            #attn_gate = self.ag_attn(x=x2_new, g=x1_expand)  # B C H W\n",
    "\n",
    "            cat_linear_x = x1_expand + x2_new  # B C H W\n",
    "            cat_linear_x = cat_linear_x.permute(0, 2, 3, 1)  # B H W C\n",
    "            cat_linear_x = self.ag_attn_norm(cat_linear_x)  # B H W C\n",
    "\n",
    "            cat_linear_x = cat_linear_x.permute(0, 3, 1, 2).contiguous()  # B C H W\n",
    "\n",
    "            tran_layer_1 = self.layer_lka_1(cat_linear_x)\n",
    "            \n",
    "            prompt_layer_1 = self.prompt1(tran_layer_1)\n",
    "            \n",
    "            cat_input_prompt = torch.cat([tran_layer_1, prompt_layer_1], dim= 1)\n",
    "            cat_input_prompt = self.noise_level1(cat_input_prompt)\n",
    "            refined_feature = self.reduce_noise_level1(cat_input_prompt)\n",
    "\n",
    "            tran_layer_2 = self.layer_lka_2(refined_feature)\n",
    "\n",
    "            tran_layer_2 = tran_layer_2.view(tran_layer_2.size(0), tran_layer_2.size(3) * tran_layer_2.size(2),\n",
    "                                             tran_layer_2.size(1))\n",
    "            if self.last_layer:\n",
    "                out = self.last_layer(\n",
    "                    self.layer_up(tran_layer_2).view(b2, 4 * h2, 4 * w2, -1).permute(0, 3, 1, 2))  # 1 9 224 224\n",
    "            else:\n",
    "                out = self.layer_up(tran_layer_2)  # 1 3136 160\n",
    "        else:\n",
    "            out = self.layer_up(x1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_0 = torch.randn(1, 96, 56, 56).cuda() # skip 3\n",
    "input_1 = torch.randn(1, 192, 28, 28).cuda() # skip 2\n",
    "input_2 = torch.randn(1, 384, 14, 14).cuda() # skip 1\n",
    "input_3 = torch.randn(1, 768, 7, 7).cuda() # X\n",
    "\n",
    "\n",
    "\n",
    "decoder_3 = MyDecoderLayerLKA_Prompt(input_size=(7,7),\n",
    "                               in_out_chan=([768, 768])).cuda()\n",
    "\n",
    "# decoder_2 = MyDecoderLayerLKA_Prompt(input_size=(14,14),\n",
    "#                                in_out_chan=([384, 384])).cuda()\n",
    "# decoder_1 = MyDecoderLayerLKA_Prompt(input_size=(28,28),\n",
    "#                                in_out_chan=([192, 192])).cuda()\n",
    "# decoder_0 = MyDecoderLayerLKA_Prompt(input_size=(56, 56), \n",
    "#                               in_out_chan=([96, 96]),\n",
    "#                               is_last=True).cuda()\n",
    "\n",
    "b, c, _, _ = input_3.shape\n",
    "\n",
    "# output_3 = decoder_3(input_3.permute(0, 2, 3, 1).view(b,-1, c))\n",
    "# output_2 = decoder_2(output_3, input_2.permute(0, 2 , 3, 1))\n",
    "# output_1 = decoder_1(output_2, input_1.permute(0, 2, 3, 1))\n",
    "# output_0 = decoder_0(output_1, input_0.permute(0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_params_in_millions(model):\n",
    "  \"\"\"Calculates the number of parameters in a PyTorch model in millions.\n",
    "\n",
    "  Args:\n",
    "    model: A PyTorch model.\n",
    "\n",
    "  Returns:\n",
    "    The number of parameters in millions.\n",
    "  \"\"\"\n",
    "  num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "  return num_params / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.128"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_params_in_millions(decoder_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the output 3 shape: torch.Size([1, 196, 384])\n",
      "the output 2 shape: torch.Size([1, 784, 192])\n"
     ]
    }
   ],
   "source": [
    "print(f\"the output 3 shape: {output_3.shape}\\nthe output 2 shape: {output_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL1402",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
