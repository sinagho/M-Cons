{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add the DCT into the Prompt component generation.\n",
    "- Make the Frequency-driven Denoising Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\DL1402\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.nn import functional as F\n",
    "from timm.models.layers import DropPath, to_2tuple\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils for DCT and Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_indices(method):\n",
    "    assert method in ['top1','top2','top4','top8','top16','top32',\n",
    "                      'bot1','bot2','bot4','bot8','bot16','bot32',\n",
    "                      'low1','low2','low4','low8','low16','low32']\n",
    "    num_freq = int(method[3:])\n",
    "    if 'top' in method:\n",
    "        all_top_indices_x = [0,0,6,0,0,1,1,4,5,1,3,0,0,0,3,2,4,6,3,5,5,2,6,5,5,3,3,4,2,2,6,1]\n",
    "        all_top_indices_y = [0,1,0,5,2,0,2,0,0,6,0,4,6,3,5,2,6,3,3,3,5,1,1,2,4,2,1,1,3,0,5,3]\n",
    "        mapper_x = all_top_indices_x[:num_freq]\n",
    "        mapper_y = all_top_indices_y[:num_freq]\n",
    "    elif 'low' in method:\n",
    "        all_low_indices_x = [0,0,1,1,0,2,2,1,2,0,3,4,0,1,3,0,1,2,3,4,5,0,1,2,3,4,5,6,1,2,3,4]\n",
    "        all_low_indices_y = [0,1,0,1,2,0,1,2,2,3,0,0,4,3,1,5,4,3,2,1,0,6,5,4,3,2,1,0,6,5,4,3]\n",
    "        mapper_x = all_low_indices_x[:num_freq]\n",
    "        mapper_y = all_low_indices_y[:num_freq]\n",
    "    elif 'bot' in method:\n",
    "        all_bot_indices_x = [6,1,3,3,2,4,1,2,4,4,5,1,4,6,2,5,6,1,6,2,2,4,3,3,5,5,6,2,5,5,3,6]\n",
    "        all_bot_indices_y = [6,4,4,6,6,3,1,4,4,5,6,5,2,2,5,1,4,3,5,0,3,1,1,2,4,2,1,1,5,3,3,3]\n",
    "        mapper_x = all_bot_indices_x[:num_freq]\n",
    "        mapper_y = all_bot_indices_y[:num_freq]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return mapper_x, mapper_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 29, 28, 14, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2wh = dict([(96,[56, 29]), (192,28), (384,14) ,(768,7)])\n",
    "dims = [96, 192, 384, 768]\n",
    "\n",
    "c2wh[96][0], c2wh[96][1], c2wh[192], c2wh[384], c2wh[768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiSpectralAttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, channel, dct_h, dct_w, reduction = 16, freq_sel_method = 'top16'):\n",
    "        super(MultiSpectralAttentionLayer, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.dct_h = dct_h\n",
    "        self.dct_w = dct_w\n",
    "\n",
    "        mapper_x, mapper_y = get_freq_indices(freq_sel_method)\n",
    "        self.num_split = len(mapper_x)\n",
    "        mapper_x = [temp_x * (dct_h // 7) for temp_x in mapper_x] \n",
    "        mapper_y = [temp_y * (dct_w // 7) for temp_y in mapper_y]\n",
    "        # make the frequencies in different sizes are identical to a 7x7 frequency space\n",
    "        # eg, (2,2) in 14x14 is identical to (1,1) in 7x7\n",
    "\n",
    "        self.dct_layer = MultiSpectralDCTLayer(dct_h, dct_w, mapper_x, mapper_y, channel)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        n,c,h,w = x.shape\n",
    "        x_pooled = x\n",
    "        if h != self.dct_h or w != self.dct_w:\n",
    "            x_pooled = torch.nn.functional.adaptive_avg_pool2d(x, (self.dct_h, self.dct_w))\n",
    "            # print(x_pooled.shape)\n",
    "            # If you have concerns about one-line-change, don't worry.   :)\n",
    "            # In the ImageNet models, this line will never be triggered. \n",
    "            # This is for compatibility in instance segmentation and object detection.\n",
    "        y = self.dct_layer(x_pooled)\n",
    "        print(y.shape)\n",
    "\n",
    "        y = self.fc(y).view(n, c, 1, 1)\n",
    "        print(y.shape)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class MultiSpectralDCTLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Generate dct filters\n",
    "    \"\"\"\n",
    "    def __init__(self, height, width, mapper_x, mapper_y, channel):\n",
    "        super(MultiSpectralDCTLayer, self).__init__()\n",
    "        \n",
    "        assert len(mapper_x) == len(mapper_y)\n",
    "        assert channel % len(mapper_x) == 0\n",
    "\n",
    "        self.num_freq = len(mapper_x)\n",
    "\n",
    "        # fixed DCT init\n",
    "        self.register_buffer('weight', self.get_dct_filter(height, width, mapper_x, mapper_y, channel))\n",
    "        \n",
    "        # fixed random init\n",
    "        # self.register_buffer('weight', torch.rand(channel, height, width))\n",
    "\n",
    "        # learnable DCT init\n",
    "        # self.register_parameter('weight', self.get_dct_filter(height, width, mapper_x, mapper_y, channel))\n",
    "        \n",
    "        # learnable random init\n",
    "        # self.register_parameter('weight', torch.rand(channel, height, width))\n",
    "\n",
    "        # num_freq, h, w\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 4, 'x must been 4 dimensions, but got ' + str(len(x.shape))\n",
    "        # n, c, h, w = x.shape\n",
    "\n",
    "        x = x * self.weight\n",
    "\n",
    "        result = torch.sum(x, dim=[2,3])\n",
    "        return result\n",
    "\n",
    "    def build_filter(self, pos, freq, POS):\n",
    "        result = math.cos(math.pi * freq * (pos + 0.5) / POS) / math.sqrt(POS) \n",
    "        if freq == 0:\n",
    "            return result\n",
    "        else:\n",
    "            return result * math.sqrt(2)\n",
    "    \n",
    "    def get_dct_filter(self, tile_size_x, tile_size_y, mapper_x, mapper_y, channel):\n",
    "        dct_filter = torch.zeros(channel, tile_size_x, tile_size_y)\n",
    "\n",
    "        c_part = channel // len(mapper_x)\n",
    "\n",
    "        for i, (u_x, v_y) in enumerate(zip(mapper_x, mapper_y)):\n",
    "            for t_x in range(tile_size_x):\n",
    "                for t_y in range(tile_size_y):\n",
    "                    dct_filter[i * c_part: (i+1)*c_part, t_x, t_y] = self.build_filter(t_x, u_x, tile_size_x) * self.build_filter(t_y, v_y, tile_size_y)\n",
    "                        \n",
    "        return dct_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 29\n",
      "torch.Size([1, 96])\n",
      "torch.Size([1, 96, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96, 56, 56])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2wh = dict([(96,[56, 29]), (192,28), (384,14) ,(768,7)])\n",
    "print(c2wh[96][0], c2wh[96][1])\n",
    "model = MultiSpectralAttentionLayer(channel= 96,\n",
    "                                    dct_h = c2wh[96][0],\n",
    "                                    dct_w= c2wh[96][1])\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "model(torch.randn(1,96,56,56).cuda()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreqPromptGenBlock(nn.Module):\n",
    "    def __init__(self, dct_h, dct_w, prompt_dim=48,prompt_len=5,prompt_size = 96,lin_dim = 192, freq_sel_method = 'top16'):\n",
    "        super(FreqPromptGenBlock,self).__init__()\n",
    "\n",
    "        self.dct_h = dct_h\n",
    "        self.dct_w = dct_w\n",
    "\n",
    "        mapper_x, mapper_y = get_freq_indices(freq_sel_method)\n",
    "        self.num_split = len(mapper_x)\n",
    "        mapper_x = [temp_x * (dct_h // 7) for temp_x in mapper_x] \n",
    "        mapper_y = [temp_y * (dct_w // 7) for temp_y in mapper_y]\n",
    "\n",
    "        self.prompt_param = nn.Parameter(torch.rand(1,prompt_len,prompt_dim,prompt_size,prompt_size))\n",
    "        self.dct_layer = MultiSpectralDCTLayer(dct_h, dct_w, mapper_x, mapper_y, prompt_dim)\n",
    "\n",
    "        self.linear_layer = nn.Linear(lin_dim,prompt_len)\n",
    "        self.conv3x3 = nn.Conv2d(prompt_dim,prompt_dim,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        B,C,H,W = x.shape\n",
    "\n",
    "        emb = self.dct_layer(x) # GAP\n",
    "        # print(emb_2.shape)\n",
    "        # emb = x.mean(dim=(-2,-1))  \n",
    "        print(emb.shape)\n",
    "\n",
    "        prompt_weights = F.softmax(self.linear_layer(emb),dim=1)\n",
    "        print(prompt_weights.shape)\n",
    "\n",
    "        prompt = prompt_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * self.prompt_param.unsqueeze(0).repeat(B,1,1,1,1,1).squeeze(1)\n",
    "        prompt = torch.sum(prompt,dim=1)\n",
    "        prompt = F.interpolate(prompt,(H,W),mode=\"bilinear\")\n",
    "        prompt = self.conv3x3(prompt)\n",
    "        \n",
    "\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384])\n",
      "torch.Size([1, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384, 14, 14])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = FreqPromptGenBlock(dct_h = c2wh[384],\n",
    "                        dct_w = c2wh[384],\n",
    "                        prompt_dim =384,\n",
    "                        prompt_len =5,\n",
    "                        prompt_size =384,\n",
    "                        lin_dim = 384).cuda()\n",
    "\n",
    "prompt(torch.randn(1, 384, 14, 14).cuda()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreqLightWeightPromptGenBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dct_h,\n",
    "                 dct_w,\n",
    "                 input_size,  \n",
    "                 prompt_dim=48, \n",
    "                 prompt_len=5, \n",
    "                 lin_dim = 192,\n",
    "                 freq_sel_method = 'top16'):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.dct_h = dct_h\n",
    "        self.dct_w = dct_w\n",
    "        input_size_w = input_size // 2 + 1\n",
    "\n",
    "        mapper_x, mapper_y = get_freq_indices(freq_sel_method)\n",
    "        self.num_split = len(mapper_x)\n",
    "        mapper_x = [temp_x * (dct_h // 7) for temp_x in mapper_x] \n",
    "        mapper_y = [temp_y * (dct_w // 7) for temp_y in mapper_y]\n",
    "\n",
    "        self.prompt_param = nn.Parameter(torch.rand(1,prompt_len,prompt_dim, input_size, input_size_w)) # B, N , C, H, (W//2+1)\n",
    "\n",
    "        self.dct_layer = MultiSpectralDCTLayer(dct_h, dct_w, mapper_x, mapper_y, prompt_dim)\n",
    "        \n",
    "        self.linear_layer = nn.Linear(lin_dim,prompt_len)\n",
    "\n",
    "        self.conv3x3 = nn.Conv2d(prompt_dim,prompt_dim,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        B,C,H,W = x.shape\n",
    "\n",
    "        w = (W // 2) + 1\n",
    "        emb = self.dct_layer(x)\n",
    "        print(emb.shape)\n",
    "        # emb = x.mean(dim=(-2,-1)) # B, C (Simple GAP)\n",
    "\n",
    "        prompt_weights = F.softmax(self.linear_layer(emb),dim=1) # B, C , C = 5\n",
    "        \n",
    "        p1 = prompt_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        print(p1.shape)\n",
    "\n",
    "        prompt = prompt_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * self.prompt_param.unsqueeze(0).repeat(B,1,1,1,1,1).squeeze(1)\n",
    "\n",
    "        p2 = self.prompt_param.unsqueeze(0).repeat(B,1,1,1,1,1).squeeze(1)\n",
    "        print(p2.shape)\n",
    "        print(prompt.shape)\n",
    "        prompt = torch.sum(prompt,dim=1)\n",
    "        prompt = F.interpolate(prompt,(H,w),mode=\"bilinear\") # B, N, C, (W//2 + 1)\n",
    "        prompt = self.conv3x3(prompt)\n",
    "        \n",
    "\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96])\n",
      "torch.Size([1, 5, 1, 1, 1])\n",
      "torch.Size([1, 5, 96, 56, 29])\n",
      "torch.Size([1, 5, 96, 56, 29])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96, 56, 29])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = FreqLightWeightPromptGenBlock(dct_h = c2wh[96][0],\n",
    "                        dct_w = c2wh[96][0],\n",
    "                        input_size=56,\n",
    "                        prompt_dim =96,\n",
    "                        prompt_len =5,\n",
    "                        lin_dim = 96).cuda()\n",
    "\n",
    "prompt(torch.randn(1, 96, 56, 56).cuda()).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class frequenctSpecificPromptGenetator(nn.Module):\n",
    "    def __init__(self, dim=3,h=128,w=65, flag_highF=False):\n",
    "        super().__init__()\n",
    "        self.flag_highF = flag_highF\n",
    "        k_size = 3\n",
    "        if flag_highF:\n",
    "            w = (w - 1) * 2\n",
    "            self.w = w\n",
    "            self.h = h\n",
    "            self.weight = nn.Parameter(torch.randn(1,dim, h, w, dtype=torch.float32) * 0.02)\n",
    "            self.body = nn.Sequential(nn.Conv2d(dim, dim, (1,k_size), padding=(0, k_size//2), groups=dim),\n",
    "                                      nn.Conv2d(dim, dim, (k_size,1), padding=(k_size//2, 0), groups=dim),\n",
    "                                      nn.GELU())\n",
    "        else:\n",
    "            self.complex_weight = nn.Parameter(torch.randn(1,dim, h, w, 2, dtype=torch.float32) * 0.02)\n",
    "            self.body = nn.Sequential(nn.Conv2d(2*dim,2*dim,kernel_size=1,stride=1),\n",
    "                                    nn.GELU(),\n",
    "                                    )\n",
    "            \n",
    "\n",
    "    def forward(self, ffm, H, W):\n",
    "        # if self.flag_highF:\n",
    "        #     ffm = F.interpolate(ffm, size=(H, W), mode='bilinear')\n",
    "        #     y_att = self.body(ffm)\n",
    "\n",
    "        #     y_f = y_att * ffm\n",
    "        #     y = y_f * self.weight\n",
    "\n",
    "        \n",
    "        ffm = F.interpolate(ffm, size=(H, W), mode='bicubic')\n",
    "        y = torch.fft.rfft2(ffm.to(torch.float32).cuda())\n",
    "        print(y.shape)\n",
    "\n",
    "        y_imag = y.imag\n",
    "\n",
    "        print(y_imag.shape)\n",
    "        y_real = y.real\n",
    "        print(y_real.shape)\n",
    "        y_f = torch.cat([y_real, y_imag], dim=1)\n",
    "\n",
    "        print(y_f.shape)\n",
    "\n",
    "        weight = torch.complex(self.complex_weight[..., 0],self.complex_weight[..., 1])\n",
    "        print(\"shape is : \", weight.shape)\n",
    "        \n",
    "        y_att = self.body(y_f)\n",
    "        print(y_att.shape)\n",
    "\n",
    "        y_f = y_f * y_att\n",
    "        print(y_f.shape)\n",
    "        \n",
    "        y_real, y_imag = torch.chunk(y_f, 2, dim=1)\n",
    "        print(y_real.shape, y_imag.shape)\n",
    "        y = torch.complex(y_real, y_imag)\n",
    "        y = y * weight\n",
    "        y = torch.fft.irfft2(y, s=(H, W))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96, 56, 29])\n",
      "torch.Size([1, 96, 56, 29])\n",
      "torch.Size([1, 96, 56, 29])\n",
      "torch.Size([1, 192, 56, 29])\n",
      "shape is :  torch.Size([1, 96, 56, 29])\n",
      "torch.Size([1, 192, 56, 29])\n",
      "torch.Size([1, 192, 56, 29])\n",
      "torch.Size([1, 96, 56, 29]) torch.Size([1, 96, 56, 29])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96, 56, 56])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = frequenctSpecificPromptGenetator(dim = 96,\n",
    "                                              h = 56,\n",
    "                                              w = 29).cuda()\n",
    "\n",
    "test_model(torch.randn(1,96,56,56), 56, 56).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourierUnit(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, groups=1):\n",
    "        super(FourierUnit, self).__init__()\n",
    "        self.groups = groups\n",
    "        self.conv_layer = torch.nn.Conv2d(in_channels=in_channels * 2, out_channels=out_channels * 2,\n",
    "                                          kernel_size=1, stride=1, padding=0, groups=self.groups, bias=False)\n",
    "        self.bn = torch.nn.BatchNorm2d(out_channels * 2)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, c, h, w = x.size()\n",
    "\n",
    "        # (batch, c, h, w/2+1, 2)\n",
    "        ffted = torch.fft.rfft2(x, norm='ortho')\n",
    "        x_fft_real = torch.unsqueeze(torch.real(ffted), dim=-1)\n",
    "        x_fft_imag = torch.unsqueeze(torch.imag(ffted), dim=-1)\n",
    "        ffted = torch.cat((x_fft_real, x_fft_imag), dim=-1)\n",
    "        # (batch, c, 2, h, w/2+1)\n",
    "        ffted = ffted.permute(0, 1, 4, 2, 3).contiguous()\n",
    "        ffted = ffted.view((batch, -1,) + ffted.size()[3:])\n",
    "\n",
    "        ffted = self.conv_layer(ffted)  # (batch, c*2, h, w/2+1)\n",
    "        ffted = self.relu(self.bn(ffted))\n",
    "\n",
    "        ffted = ffted.view((batch, -1, 2,) + ffted.size()[2:]).permute(\n",
    "            0, 1, 3, 4, 2).contiguous()  # (batch,c, t, h, w/2+1, 2)\n",
    "        ffted = torch.view_as_complex(ffted)\n",
    "\n",
    "        output = torch.fft.irfft2(ffted, s=(h, w), norm='ortho')\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96, 56, 56])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 96\n",
    "\n",
    "ffc = FourierUnit(in_channels=dim, out_channels= dim)\n",
    "\n",
    "ffc(torch.randn(1,96,56,56)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_params_in_millions(model):\n",
    "  \"\"\"Calculates the number of parameters in a PyTorch model in millions.\n",
    "\n",
    "  Args:\n",
    "    model: A PyTorch model.\n",
    "\n",
    "  Returns:\n",
    "    The number of parameters in millions.\n",
    "  \"\"\"\n",
    "  num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "  return num_params / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreqLightWeightPromptGenBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dct_h,\n",
    "                 dct_w,\n",
    "                 input_size,  \n",
    "                 prompt_dim=48, \n",
    "                 prompt_len=5, \n",
    "                 lin_dim = 192,\n",
    "                 freq_sel_method = 'top16'):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.dct_h = dct_h\n",
    "        self.dct_w = dct_w\n",
    "        input_size_w = input_size // 2 + 1\n",
    "\n",
    "        mapper_x, mapper_y = get_freq_indices(freq_sel_method)\n",
    "        self.num_split = len(mapper_x)\n",
    "        mapper_x = [temp_x * (dct_h // 7) for temp_x in mapper_x] \n",
    "        mapper_y = [temp_y * (dct_w // 7) for temp_y in mapper_y]\n",
    "\n",
    "        self.prompt_param = nn.Parameter(torch.rand(1,prompt_len,prompt_dim, input_size, input_size_w, 2)) # B, N , C, H, (W//2+1)\n",
    "\n",
    "        self.dct_layer = MultiSpectralDCTLayer(dct_h, dct_w, mapper_x, mapper_y, prompt_dim)\n",
    "        \n",
    "        self.linear_layer = nn.Linear(lin_dim,prompt_len)\n",
    "\n",
    "        self.conv3x3 = nn.Conv3d(prompt_dim,prompt_dim,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        B,C,H,W = x.shape\n",
    "\n",
    "        w = (W // 2) + 1\n",
    "        emb = self.dct_layer(x)\n",
    "        #print(emb.shape)\n",
    "        # emb = x.mean(dim=(-2,-1)) # B, C (Simple GAP)\n",
    "\n",
    "        prompt_weights = F.softmax(self.linear_layer(emb),dim=1) # B, C , C = 5\n",
    "        \n",
    "        p1 = prompt_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        #print(p1.shape)\n",
    "        # print(self.prompt_param.unsqueeze(0).repeat(B,1,1,1,1,1, 1).squeeze(1).shape)\n",
    "        prompt = prompt_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * self.prompt_param.unsqueeze(0).repeat(B,1,1,1,1,1, 1).squeeze(1)\n",
    "\n",
    "        # p2 = self.prompt_param.unsqueeze(0).repeat(B,1,1,1,1,1).squeeze(1)\n",
    "        # print(p2.shape)\n",
    "        #print(prompt.shape)\n",
    "        prompt = torch.sum(prompt,dim=1)\n",
    "        #print(prompt.shape)\n",
    "        prompt = F.interpolate(prompt,(H,w, 2),mode=\"trilinear\") # B, N, C, (W//2 + 1)\n",
    "        prompt = self.conv3x3(prompt)\n",
    "        \n",
    "\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 96, 56, 29, 2]), 1.808357)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = FreqLightWeightPromptGenBlock(dct_h = c2wh[96][0],\n",
    "                        dct_w = c2wh[96][0],\n",
    "                        input_size=56,\n",
    "                        prompt_dim =96,\n",
    "                        prompt_len =5,\n",
    "                        lin_dim = 96).cuda()\n",
    "\n",
    "prompt(torch.randn(1, 96, 56, 56).cuda()).shape, calculate_params_in_millions(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreqPromptGenBlock(nn.Module):\n",
    "    def __init__(self, dct_h, dct_w, prompt_dim=48, prompt_len=5, prompt_size = 96, lin_dim = 192, freq_sel_method = 'top16'):\n",
    "        super(FreqPromptGenBlock,self).__init__()\n",
    "\n",
    "        self.dct_h = dct_h\n",
    "        self.dct_w = dct_w\n",
    "\n",
    "        mapper_x, mapper_y = get_freq_indices(freq_sel_method)\n",
    "        self.num_split = len(mapper_x)\n",
    "        mapper_x = [temp_x * (dct_h // 7) for temp_x in mapper_x] \n",
    "        mapper_y = [temp_y * (dct_w // 7) for temp_y in mapper_y]\n",
    "\n",
    "        self.prompt_param = nn.Parameter(torch.rand(1,prompt_len,prompt_dim,prompt_size,prompt_size))\n",
    "        self.dct_layer = MultiSpectralDCTLayer(dct_h, dct_w, mapper_x, mapper_y, prompt_dim)\n",
    "\n",
    "        self.linear_layer = nn.Linear(lin_dim,prompt_len)\n",
    "        self.conv3x3 = nn.Conv2d(prompt_dim,prompt_dim,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        B,C,H,W = x.shape\n",
    "        w = (W // 2) + 1\n",
    "\n",
    "        emb = self.dct_layer(x) # GAP\n",
    "        # print(emb_2.shape)\n",
    "        # emb = x.mean(dim=(-2,-1))  \n",
    "        print(emb.shape)\n",
    "\n",
    "        prompt_weights = F.softmax(self.linear_layer(emb),dim=1)\n",
    "        print(prompt_weights.shape)\n",
    "\n",
    "        pr_ = self.prompt_param.unsqueeze(0).repeat(B,1,1,1,1,1).squeeze(1)\n",
    "        print(pr_.shape)\n",
    "        prompt = prompt_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * self.prompt_param.unsqueeze(0).repeat(B,1,1,1,1,1).squeeze(1)\n",
    "        prompt = torch.sum(prompt,dim=1)\n",
    "        prompt = F.interpolate(prompt,(H,w),mode=\"bilinear\")\n",
    "        prompt = self.conv3x3(prompt)\n",
    "        \n",
    "\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96, 56, 29, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr2 = FreqPromptGenBlock(dct_h = c2wh[96][0],\n",
    "                        dct_w = c2wh[96][0],\n",
    "                        prompt_dim =384,\n",
    "                        prompt_len =5,\n",
    "                        prompt_size =384,\n",
    "                        lin_dim = 384).cuda()\n",
    "\n",
    "prompt(torch.randn(1, 96, 56, 56).cuda()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrqRefiner(nn.Module):\n",
    "    def __init__(self, dim=3,h=128,w=128):\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        # w = w//2 + 1\n",
    "        self.complex_weights = FreqLightWeightPromptGenBlock(dct_h= h,\n",
    "                                                             dct_w= h,\n",
    "                                                             input_size=h,\n",
    "                                                             prompt_dim=dim,\n",
    "                                                             prompt_len=5,\n",
    "                                                             lin_dim=dim)\n",
    "        #self.complex_weight = nn.Parameter(torch.randn(1,dim, h, w, 2, dtype=torch.float32) * 0.02)\n",
    "        self.body = nn.Sequential(nn.Conv2d(2*dim,2*dim,kernel_size=1,stride=1),\n",
    "                                    nn.GELU())\n",
    "        # self.kv_conv = nn.Sequential(nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, stride=1, padding=1))\n",
    "\n",
    "    def forward(self, x, H = None, W = None):\n",
    "        # if self.flag_highF:\n",
    "        #     ffm = F.interpolate(ffm, size=(H, W), mode='bilinear')\n",
    "        #     y_att = self.body(ffm)\n",
    "\n",
    "        #     y_f = y_att * ffm\n",
    "        #     y = y_f * self.weight\n",
    "        if self.h != None and self.w != None:\n",
    "            H = self.h\n",
    "            W = self.w\n",
    "        \n",
    "        x = F.interpolate(x, size=(H, W), mode='bicubic')\n",
    "        y = torch.fft.rfft2(x.to(torch.float32).cuda())\n",
    "        # print(y.shape)\n",
    "\n",
    "        y_imag = y.imag\n",
    "\n",
    "        # print(y_imag.shape)\n",
    "        y_real = y.real\n",
    "        # print(y_real.shape)\n",
    "        y_f = torch.cat([y_real, y_imag], dim=1)\n",
    "\n",
    "        # print(y_f.shape)\n",
    "\n",
    "        ## Weight Making ##\n",
    " \n",
    "        weight = torch.complex(self.complex_weights(x)[..., 0],self.complex_weights(x)[..., 1])\n",
    "\n",
    "        ########\n",
    "        # print(self.complex_weights(x)[..., 0].shape)\n",
    "        # print(weight.shape)\n",
    "        # print(\"shape is : \", weight.shape)\n",
    "        \n",
    "        y_att = self.body(y_f)\n",
    "        # print(y_att.shape)\n",
    "\n",
    "        y_f = y_f * y_att\n",
    "        # print(y_f.shape)\n",
    "        \n",
    "        y_real, y_imag = torch.chunk(y_f, 2, dim=1)\n",
    "        # print(y_real.shape, y_imag.shape)\n",
    "        y = torch.complex(y_real, y_imag)\n",
    "        y = y * weight\n",
    "        y = torch.fft.irfft2(y, s=(H, W))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.845413"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = FrqRefiner(dim = 96,\n",
    "                        h = 56,\n",
    "                        w = 56).cuda()\n",
    "\n",
    "output = test_model(torch.randn(1,96,56,56).cuda())\n",
    "calculate_params_in_millions(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProjection(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0., bias=True, isQuery = True):\n",
    "        super().__init__()\n",
    "        self.isQuery =isQuery\n",
    "        inner_dim = dim_head *  heads\n",
    "        self.heads = heads\n",
    "        if self.isQuery:\n",
    "            self.to_q = nn.Linear(dim, inner_dim, bias = bias)\n",
    "        else:\n",
    "            self.to_kv = nn.Linear(dim, 2*inner_dim, bias = bias)\n",
    "        self.dim = dim\n",
    "        self.inner_dim = inner_dim\n",
    "\n",
    "    def forward(self, x, attn_kv=None):\n",
    "        B_, N, C = x.shape\n",
    "        if attn_kv is not None:\n",
    "            attn_kv = attn_kv.unsqueeze(0).repeat(B_,1,1)\n",
    "        else:\n",
    "            attn_kv = x\n",
    "        N_kv = attn_kv.size(1)\n",
    "        if self.isQuery:\n",
    "            q = self.to_q(x).reshape(B_, N, 1, self.heads, C // self.heads).permute(2, 0, 3, 1, 4).contiguous()\n",
    "            q = q[0]\n",
    "            return q\n",
    "        else:\n",
    "            C = self.inner_dim \n",
    "            kv = self.to_kv(attn_kv).reshape(B_, N_kv, 2, self.heads, C // self.heads).permute(2, 0, 3, 1, 4).contiguous()\n",
    "            k, v = kv[0], kv[1] \n",
    "            return k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, win_size, dilation_rate=1):\n",
    "    B, H, W, C = x.shape\n",
    "    if dilation_rate !=1:\n",
    "        x = x.permute(0,3,1,2) # B, C, H, W\n",
    "        assert type(dilation_rate) is int, 'dilation_rate should be a int'\n",
    "        x = F.unfold(x, kernel_size=win_size,dilation=dilation_rate,padding=4*(dilation_rate-1),stride=win_size) # B, C*Wh*Ww, H/Wh*W/Ww\n",
    "        windows = x.permute(0,2,1).contiguous().view(-1, C, win_size, win_size) # B' ,C ,Wh ,Ww\n",
    "        windows = windows.permute(0,2,3,1).contiguous() # B' ,Wh ,Ww ,C\n",
    "    else:\n",
    "        x = x.view(B, H // win_size, win_size, W // win_size, win_size, C)\n",
    "        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, win_size, win_size, C) # B' ,Wh ,Ww ,C\n",
    "    return windows\n",
    "\n",
    "def window_reverse(windows, win_size, H, W, dilation_rate=1):\n",
    "    # B' ,Wh ,Ww ,C\n",
    "    B = int(windows.shape[0] / (H * W / win_size / win_size))\n",
    "    x = windows.view(B, H // win_size, W // win_size, win_size, win_size, -1)\n",
    "    if dilation_rate !=1:\n",
    "        x = windows.permute(0,5,3,4,1,2).contiguous() # B, C*Wh*Ww, H/Wh*W/Ww\n",
    "        x = F.fold(x, (H, W), kernel_size=win_size, dilation=dilation_rate, padding=4*(dilation_rate-1),stride=win_size)\n",
    "    else:\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyPromptFusion(nn.Module):\n",
    "    def __init__(self, dim, dim_bak,win_size, num_heads, qkv_bias=True, qk_scale=None, bias=False):\n",
    "        super(FrequencyPromptFusion, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.win_size = win_size  # Wh, Ww\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.to_q = LinearProjection(dim,num_heads,dim//num_heads,bias=qkv_bias,isQuery=True)\n",
    "        self.to_kv = LinearProjection(dim_bak,num_heads,dim//num_heads,bias=qkv_bias,isQuery=False)\n",
    "        \n",
    "        self.kv_dwconv = nn.Conv2d(dim_bak , dim_bak, kernel_size=3, stride=1, padding=1, groups=dim_bak, bias=bias)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.project_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, query_feature, key_value_feature):\n",
    "\n",
    "        b,c,h,w = query_feature.shape\n",
    "        _,c_2,_,_ = key_value_feature.shape\n",
    "        \n",
    "        key_value_feature = self.kv_dwconv(key_value_feature)\n",
    "        \n",
    "        # partition windows\n",
    "        query_feature = rearrange(query_feature, ' b c1 h w -> b h w c1 ', h=h, w=w)\n",
    "        query_feature_windows = window_partition(query_feature, self.win_size)  # nW*B, win_size, win_size, C  N*C->C\n",
    "        query_feature_windows = query_feature_windows.view(-1, self.win_size * self.win_size, c)  # nW*B, win_size*win_size, C\n",
    "        \n",
    "        key_value_feature = rearrange(key_value_feature, ' b c2 h w -> b h w c2 ', h=h, w=w)\n",
    "        key_value_feature_windows = window_partition(key_value_feature, self.win_size)  # nW*B, win_size, win_size, C  N*C->C\n",
    "        key_value_feature_windows = key_value_feature_windows.view(-1, self.win_size * self.win_size, c_2)  # nW*B, win_size*win_size, C\n",
    "        \n",
    "        B_, N, C = query_feature_windows.shape\n",
    "        \n",
    "        query = self.to_q(query_feature_windows)\n",
    "        query = query * self.scale\n",
    "        \n",
    "        key,value = self.to_kv(key_value_feature_windows)\n",
    "        attn = (query @ key.transpose(-2, -1).contiguous())\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        out = (attn @ value).transpose(1, 2).contiguous().reshape(B_, N, C)\n",
    "\n",
    "        out = self.project_out(out)\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = out.view(-1, self.win_size, self.win_size, C)\n",
    "        attn_windows = window_reverse(attn_windows, self.win_size, h, w)  # B H' W' C\n",
    "        return rearrange(attn_windows, 'b h w c -> b c h w', h=h, w=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 96, 56, 56]), 0.038112)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuse = FrequencyPromptFusion(dim = 96, dim_bak=96, win_size=8, num_heads= 2).cuda()\n",
    "query_feature = torch.randn(1,96,56,56).cuda()\n",
    "fuse(query_feature, output).shape, calculate_params_in_millions(fuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrequencyPromptFusion(\n",
       "  (to_q): LinearProjection(\n",
       "    (to_q): Linear(in_features=96, out_features=96, bias=True)\n",
       "  )\n",
       "  (to_kv): LinearProjection(\n",
       "    (to_kv): Linear(in_features=96, out_features=192, bias=True)\n",
       "  )\n",
       "  (kv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "  (softmax): Softmax(dim=-1)\n",
       "  (project_out): Linear(in_features=96, out_features=96, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DWConvLKA(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super(DWConvLKA, self).__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dwconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0., linear=False):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Conv2d(in_features, hidden_features, 1)\n",
    "        self.dwconv = DWConvLKA(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Conv2d(hidden_features, out_features, 1)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.linear = linear\n",
    "        if self.linear:\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        if self.linear:\n",
    "            x = self.relu(x)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(dim, dim, 5, padding=2, groups=dim)\n",
    "        self.conv_spatial = nn.Conv2d(\n",
    "            dim, dim, 7, stride=1, padding=9, groups=dim, dilation=3)\n",
    "        self.conv1 = nn.Conv2d(dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.clone()\n",
    "        attn = self.conv0(x)\n",
    "        attn = self.conv_spatial(attn)\n",
    "        attn = self.conv1(attn)\n",
    "        return u * attn\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.proj_1 = nn.Conv2d(d_model, d_model, 1)\n",
    "        self.activation = nn.GELU()\n",
    "        self.spatial_gating_unit = AttentionModule(d_model)\n",
    "        self.proj_2 = nn.Conv2d(d_model, d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shorcut = x.clone()\n",
    "        x = self.proj_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.spatial_gating_unit(x)\n",
    "        x = self.proj_2(x)\n",
    "        x = x + shorcut\n",
    "        return x\n",
    "\n",
    "\n",
    "class LKABlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 mlp_ratio=4.,\n",
    "                 drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 linear=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)  # build_norm_layer(norm_cfg, dim)[1]\n",
    "        self.attn = SpatialAttention(dim)\n",
    "        self.drop_path = DropPath(\n",
    "            drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim)  # build_norm_layer(norm_cfg, dim)[1]\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=act_layer, drop=drop, linear=linear)\n",
    "        layer_scale_init_value = 1e-2\n",
    "        self.layer_scale_1 = nn.Parameter(\n",
    "            layer_scale_init_value * torch.ones((dim)), requires_grad=True)\n",
    "        self.layer_scale_2 = nn.Parameter(\n",
    "            layer_scale_init_value * torch.ones((dim)), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B, N, C = x.shape\n",
    "        # x = x.permute(0, 2, 1).view(B, C, H, W)\n",
    "        y = x.permute(0, 2, 3, 1)  # b h w c, because norm requires this\n",
    "        y = self.norm1(y)\n",
    "        y = y.permute(0, 3, 1, 2)  # b c h w, because attn requieres this\n",
    "        y = self.attn(y)\n",
    "        y = self.layer_scale_1.unsqueeze(-1).unsqueeze(-1) * y\n",
    "        y = self.drop_path(y)\n",
    "        x = x + y\n",
    "        # x = x + self.drop_path(self.layer_scale_1.unsqueeze(-1).unsqueeze(-1)\n",
    "        #                       * self.attn(self.norm1(x)))\n",
    "\n",
    "        y = x.permute(0, 2, 3, 1)  # b h w c, because norm requires this\n",
    "        y = self.norm2(y)\n",
    "        y = y.permute(0, 3, 1, 2)  # b c h w, because attn requieres this\n",
    "        y = self.mlp(y)\n",
    "        y = self.layer_scale_2.unsqueeze(-1).unsqueeze(-1) * y\n",
    "        y = self.drop_path(y)\n",
    "        x = x + y\n",
    "        # x = x + self.drop_path(self.layer_scale_2.unsqueeze(-1).unsqueeze(-1)\n",
    "        #                       * self.mlp(self.norm2(x)))\n",
    "        # x = x.view(B, C, N).permute(0, 2, 1)\n",
    "        # print(\"LKA return shape: {}\".format(x.shape))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExpand(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, dim_scale=2, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.expand = nn.Linear(dim, 2 * dim, bias=False) if dim_scale == 2 else nn.Identity()\n",
    "        self.norm = norm_layer(dim // dim_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        # print(\"x_shape-----\",x.shape)\n",
    "        H, W = self.input_resolution\n",
    "        x = self.expand(x)\n",
    "\n",
    "        B, L, C = x.shape\n",
    "        # print(x.shape)\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x = rearrange(x, \"b h w (p1 p2 c)-> b (h p1) (w p2) c\", p1=2, p2=2, c=C // 4)\n",
    "        x = x.view(B, -1, C // 4)\n",
    "        x = self.norm(x.clone())\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FinalPatchExpand_X4(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, dim_scale=4, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.dim_scale = dim_scale\n",
    "        self.expand = nn.Linear(dim, 16 * dim, bias=False)\n",
    "        self.output_dim = dim\n",
    "        self.norm = norm_layer(self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        x = self.expand(x)\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x = rearrange(\n",
    "            x, \"b h w (p1 p2 c)-> b (h p1) (w p2) c\", p1=self.dim_scale, p2=self.dim_scale, c=C // (self.dim_scale ** 2)\n",
    "        )\n",
    "        x = x.view(B, -1, self.output_dim)\n",
    "        x = self.norm(x.clone())\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecoderLayerLKAFreq(nn.Module):\n",
    "    def __init__(\n",
    "            self, input_size: tuple, in_out_chan: tuple, n_class=9,\n",
    "            norm_layer=nn.LayerNorm, is_last=False, decoder_prompt = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_dim = in_out_chan[0]\n",
    "        x1_dim = in_out_chan[1]\n",
    "        self.decoder_prompt = decoder_prompt\n",
    "        # prompt_ratio = prompt_ratio\n",
    "        \n",
    "        if not is_last:\n",
    "            self.x1_linear = nn.Linear(x1_dim, out_dim)\n",
    "            #self.ag_attn = MultiScaleGatedAttn(dim=x1_dim)\n",
    "            self.ag_attn_norm = nn.LayerNorm(out_dim)\n",
    "\n",
    "            self.layer_up = PatchExpand(input_resolution=input_size, dim=out_dim, dim_scale=2, norm_layer=norm_layer)\n",
    "            self.last_layer = None\n",
    "        else:\n",
    "            self.x1_linear = nn.Linear(x1_dim, out_dim)\n",
    "            #self.ag_attn = MultiScaleGatedAttn(dim=x1_dim)\n",
    "            self.ag_attn_norm = nn.LayerNorm(out_dim)\n",
    "\n",
    "            self.layer_up = FinalPatchExpand_X4(\n",
    "                input_resolution=input_size, dim=out_dim, dim_scale=4, norm_layer=norm_layer\n",
    "            )\n",
    "            self.last_layer = nn.Conv2d(out_dim, n_class, 1)\n",
    "\n",
    "        \n",
    "        self.layer_lka_1 = LKABlock(dim=out_dim)\n",
    "        ## Prompt Module must be located here.\n",
    "\n",
    "        #dim_p = int(out_dim * 0.75)\n",
    "        if decoder_prompt: \n",
    "            dim_p = out_dim\n",
    "            self.refiner = FrqRefiner(dim = dim_p,\n",
    "                                      h = input_size[0],\n",
    "                                      w = input_size[0])\n",
    "        \n",
    "            self.fused = FrequencyPromptFusion(dim = dim_p,\n",
    "                                               dim_bak= dim,\n",
    "                                               win_size= 8,\n",
    "                                               num_heads= 2)\n",
    "        \n",
    "            self.mlp = nn.Conv2d(int(dim_p),int(dim_p),kernel_size=1,bias=False)\n",
    "\n",
    "        self.layer_lka_2 = LKABlock(dim=out_dim)\n",
    "\n",
    "        def init_weights(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.LayerNorm):\n",
    "                    nn.init.ones_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.Conv2d):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "\n",
    "        init_weights(self)\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        if x2 is not None:  # skip connection exist\n",
    "            x2 = x2.contiguous()\n",
    "            # b, c, h, w = x1.shape\n",
    "            b2, h2, w2, c2 = x2.shape  # e.g: 1 28 28 320, 1 56 56 128\n",
    "            x2 = x2.view(b2, -1, c2)  # e.g: 1 784 320, 1 3136 128\n",
    "\n",
    "            x1_expand = self.x1_linear(x1)  # e.g: 1 784 256 --> 1 784 320, 1 3136 160 --> 1 3136 128\n",
    "\n",
    "            x2_new = x2.view(x2.size(0), x2.size(2), x2.size(1) // w2, x2.size(1) // h2) # B, C, H, W\n",
    "            \n",
    "\n",
    "            x1_expand = x1_expand.view(x2.size(0), x2.size(2), x2.size(1) // w2, x2.size(1) // h2) # B, C, H, W\n",
    "\n",
    "            # print(f'the x1_expand shape is: {x1_expand.shape}\\n\\t the x2_new shape is: {x2_new.shape}')\n",
    "\n",
    "            cat_linear_x = x1_expand + x2_new  # B C H W\n",
    "            cat_linear_x = cat_linear_x.permute(0, 2, 3, 1)  # B H W C\n",
    "            cat_linear_x = self.ag_attn_norm(cat_linear_x)  # B H W C\n",
    "\n",
    "            cat_linear_x = cat_linear_x.permute(0, 3, 1, 2).contiguous()  # B C H W\n",
    "\n",
    "            refined_feature = self.layer_lka_1(cat_linear_x)\n",
    "            \n",
    "            \n",
    "            if self.decoder_prompt:\n",
    "                prompt_layer_1 = self.refiner(refined_feature)\n",
    "                # cat_input_prompt = torch.cat([refined_feature, prompt_layer_1], dim= 1)\n",
    "                fused_map = self.fused(refined_feature, prompt_layer_1)\n",
    "                refined_feature = self.mlp(fused_map)\n",
    "\n",
    "            tran_layer_2 = self.layer_lka_2(refined_feature)\n",
    "\n",
    "            tran_layer_2 = tran_layer_2.view(tran_layer_2.size(0), tran_layer_2.size(3) * tran_layer_2.size(2),\n",
    "                                             tran_layer_2.size(1))\n",
    "            if self.last_layer:\n",
    "                \n",
    "                out = self.last_layer(\n",
    "                    self.layer_up(tran_layer_2).view(b2, 4 * h2, 4 * w2, -1).permute(0, 3, 1, 2))  # 1 9 224 224\n",
    "            else:\n",
    "                out = self.layer_up(tran_layer_2)  # 1 3136 160\n",
    "        else:\n",
    "            out = self.layer_up(x1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_0 = torch.randn(1, 96, 56, 56).cuda() # skip 3\n",
    "input_1 = torch.randn(1, 192, 28, 28).cuda() # skip 2\n",
    "input_2 = torch.randn(1, 384, 14, 14).cuda() # skip 1\n",
    "input_3 = torch.randn(1, 768, 7, 7).cuda() # X\n",
    "\n",
    "\n",
    "\n",
    "decoder_3 = MyDecoderLayerLKAFreq(input_size=(7,7),\n",
    "                               in_out_chan=([768, 768]), decoder_prompt=False).cuda()\n",
    "\n",
    "decoder_2 = MyDecoderLayerLKAFreq(input_size=(14,14),\n",
    "                               in_out_chan=([384, 384]),\n",
    "                               decoder_prompt=False\n",
    "                               ).cuda()\n",
    "decoder_1 = MyDecoderLayerLKAFreq(input_size=(28,28),\n",
    "                               in_out_chan=([192, 192]),\n",
    "                               decoder_prompt=False).cuda()\n",
    "decoder_0 = MyDecoderLayerLKAFreq(input_size=(56, 56), \n",
    "                              in_out_chan=([96, 96]),\n",
    "                              decoder_prompt=True,\n",
    "                              is_last=True).cuda()\n",
    "\n",
    "b, c, _, _ = input_3.shape\n",
    "\n",
    "output_3 = decoder_3(input_3.permute(0, 2, 3, 1).view(b,-1, c))\n",
    "output_2 = decoder_2(output_3, input_2.permute(0, 2 , 3, 1))\n",
    "output_1 = decoder_1(output_2, input_1.permute(0, 2, 3, 1))\n",
    "output_0 = decoder_0(output_1, input_0.permute(0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check V6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreqLightWeightPromptGenBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dct_h,\n",
    "                 dct_w,\n",
    "                 input_size,  \n",
    "                 prompt_dim=48, \n",
    "                 prompt_len=5, \n",
    "                 lin_dim = 192,\n",
    "                 freq_sel_method = 'top16'):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.dct_h = dct_h\n",
    "        self.dct_w = dct_w\n",
    "        input_size_w = input_size // 2 + 1\n",
    "\n",
    "        mapper_x, mapper_y = get_freq_indices(freq_sel_method)\n",
    "        self.num_split = len(mapper_x)\n",
    "        mapper_x = [temp_x * (dct_h // 7) for temp_x in mapper_x] \n",
    "        mapper_y = [temp_y * (dct_w // 7) for temp_y in mapper_y]\n",
    "\n",
    "        self.prompt_param = nn.Parameter(torch.rand(1,prompt_len,prompt_dim, input_size, input_size_w, 2)) # B, N , C, H, (W//2+1)\n",
    "\n",
    "        self.dct_layer = MultiSpectralDCTLayer(dct_h, dct_w, mapper_x, mapper_y, prompt_dim)\n",
    "        \n",
    "        self.linear_layer = nn.Linear(lin_dim,prompt_len)\n",
    "\n",
    "        self.conv3x3 = nn.Sequential(nn.Conv3d(prompt_dim,prompt_dim,kernel_size=3,stride=1,padding=1,bias=False),\n",
    "                                     nn.BatchNorm3d(prompt_dim))\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        B,C,H,W = x.shape\n",
    "\n",
    "        w = (W // 2) + 1\n",
    "        emb = self.dct_layer(x)\n",
    "        #print(emb.shape)\n",
    "        # emb = x.mean(dim=(-2,-1)) # B, C (Simple GAP)\n",
    "\n",
    "        prompt_weights = F.softmax(self.linear_layer(emb),dim=1) # B, C , C = 5\n",
    "        \n",
    "        p1 = prompt_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        #print(p1.shape)\n",
    "        # print(self.prompt_param.unsqueeze(0).repeat(B,1,1,1,1,1, 1).squeeze(1).shape)\n",
    "        prompt = prompt_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * self.prompt_param.unsqueeze(0).repeat(B,1,1,1,1,1, 1).squeeze(1)\n",
    "\n",
    "        # p2 = self.prompt_param.unsqueeze(0).repeat(B,1,1,1,1,1).squeeze(1)\n",
    "        # print(p2.shape)\n",
    "        #print(prompt.shape)\n",
    "        prompt = torch.sum(prompt,dim=1)\n",
    "        #print(prompt.shape)\n",
    "        prompt = F.interpolate(prompt,(H,w, 2),mode=\"trilinear\") # B, N, C, (W//2 + 1)\n",
    "        prompt = self.conv3x3(prompt)\n",
    "        \n",
    "\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrqRefinerEnhanced(nn.Module):\n",
    "    def __init__(self, dim=3,h=128,w=128):\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        # w = w//2 + 1\n",
    "        self.complex_weights = FreqLightWeightPromptGenBlock(dct_h= h,\n",
    "                                                             dct_w= h,\n",
    "                                                             input_size=h,\n",
    "                                                             prompt_dim=dim,\n",
    "                                                             prompt_len=5,\n",
    "                                                             lin_dim=dim)\n",
    "        \n",
    "        self.body = nn.Sequential(nn.Conv2d(2*dim,2*dim,kernel_size=1,stride=1),\n",
    "                                    nn.GELU())\n",
    "        \n",
    "        self.conv_enhancer = nn.Sequential(nn.Conv2d(dim,\n",
    "                                                     dim,\n",
    "                                                     kernel_size=1,\n",
    "                                                     stride=1\n",
    "                                                     ),\n",
    "                                          nn.BatchNorm2d(dim),\n",
    "                                          nn.GELU())\n",
    "        \n",
    "    def forward(self, x, H = None, W = None):\n",
    "        \n",
    "        if self.h != None and self.w != None:\n",
    "            H = self.h\n",
    "            W = self.w\n",
    "        \n",
    "        x = F.interpolate(x, size=(H, W), mode='bicubic')\n",
    "        y = torch.fft.rfft2(x.to(torch.float32).cuda())\n",
    "        # print(y.shape)\n",
    "\n",
    "        y_imag = y.imag\n",
    "\n",
    "        # print(y_imag.shape)\n",
    "        y_real = y.real\n",
    "        # print(y_real.shape)\n",
    "        y_f = torch.cat([y_real, y_imag], dim=1)\n",
    "\n",
    "        # print(y_f.shape)\n",
    "\n",
    "        ## Weight Making ##\n",
    " \n",
    "        weight = torch.complex(self.complex_weights(x)[..., 0],self.complex_weights(x)[..., 1])\n",
    "\n",
    "        ########\n",
    "        # print(self.complex_weights(x)[..., 0].shape)\n",
    "        # print(weight.shape)\n",
    "        # print(\"shape is : \", weight.shape)\n",
    "        \n",
    "        y_att = self.body(y_f)\n",
    "        # print(y_att.shape)\n",
    "\n",
    "        y_f = y_f * y_att\n",
    "        # print(y_f.shape)\n",
    "        \n",
    "        y_real, y_imag = torch.chunk(y_f, 2, dim=1)\n",
    "        # print(y_real.shape, y_imag.shape)\n",
    "        y = torch.complex(y_real, y_imag)\n",
    "        y = y * weight\n",
    "        y = torch.fft.irfft2(y, s=(H, W))\n",
    "        y = self.conv_enhancer(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyPromptFusionEnhanced(nn.Module):\n",
    "    def __init__(self, dim, dim_bak, num_heads,win_size=8, bias=False):\n",
    "        super(FrequencyPromptFusionEnhanced, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "        self.q = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "        self.ap_kv = nn.AdaptiveAvgPool2d(1)\n",
    "        self.kv = nn.Conv2d(dim_bak, dim * 2, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.project_out = nn.Sequential(nn.Conv2d( dim, dim, kernel_size=1, bias=bias),\n",
    "                                         nn.BatchNorm2d(dim))\n",
    "\n",
    "    def forward(self, feature, prompt_feature):\n",
    "        b, c1,h,w = feature.shape\n",
    "        _, c2,_,_ = prompt_feature.shape\n",
    "\n",
    "        query = self.q(feature).reshape(b, h * w, self.num_heads, c1 // self.num_heads).permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        prompt_feature = self.ap_kv(prompt_feature)#.reshape(b, c2, -1).permute(0, 2, 1)\n",
    "        key_value = self.kv(prompt_feature).reshape(b, 2*c1, -1).permute(0, 2, 1).contiguous().reshape(b, -1, 2, self.num_heads, c1 // self.num_heads).permute(2, 0, 3, 1, 4).contiguous()\n",
    "        key, value = key_value[0], key_value[1]\n",
    "\n",
    "        attn = (query @ key.transpose(-2, -1).contiguous()) * self.temperature\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        out = (attn @ value)\n",
    "\n",
    "        out = rearrange(out, 'b head (h w) c -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
    "        out = self.project_out(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 96, 56, 56]), 1.855109)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = FrqRefinerEnhanced(dim = 96, h= 56, w=56).cuda()\n",
    "output_frq = module(torch.randn(1,96,56,56).cuda())\n",
    "module(torch.randn(1,96,56,56).cuda()).shape, calculate_params_in_millions(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 96, 56, 56]), 0.037058)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuser = FrequencyPromptFusionEnhanced(dim = 96, dim_bak=96, num_heads= 2).cuda()\n",
    "fuser(torch.randn(1,96,56,56).cuda(), output_frq).shape, calculate_params_in_millions(fuser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecoderLayerLKAFreqEnhanced(nn.Module):\n",
    "    def __init__(\n",
    "            self, input_size: tuple, in_out_chan: tuple, n_class=9,\n",
    "            norm_layer=nn.LayerNorm, is_last=False, decoder_prompt = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_dim = in_out_chan[0]\n",
    "        x1_dim = in_out_chan[1]\n",
    "        self.decoder_prompt = decoder_prompt\n",
    "        # prompt_ratio = prompt_ratio\n",
    "        \n",
    "        if not is_last:\n",
    "            self.x1_linear = nn.Linear(x1_dim, out_dim)\n",
    "            #self.ag_attn = MultiScaleGatedAttn(dim=x1_dim)\n",
    "            self.ag_attn_norm = nn.LayerNorm(out_dim)\n",
    "\n",
    "            self.layer_up = PatchExpand(input_resolution=input_size, dim=out_dim, dim_scale=2, norm_layer=norm_layer)\n",
    "            self.last_layer = None\n",
    "        else:\n",
    "            self.x1_linear = nn.Linear(x1_dim, out_dim)\n",
    "            #self.ag_attn = MultiScaleGatedAttn(dim=x1_dim)\n",
    "            self.ag_attn_norm = nn.LayerNorm(out_dim)\n",
    "\n",
    "            self.layer_up = FinalPatchExpand_X4(\n",
    "                input_resolution=input_size, dim=out_dim, dim_scale=4, norm_layer=norm_layer\n",
    "            )\n",
    "            self.last_layer = nn.Conv2d(out_dim, n_class, 1)\n",
    "\n",
    "        \n",
    "        self.layer_lka_1 = LKABlock(dim=out_dim)\n",
    "        ## Prompt Module must be located here.\n",
    "\n",
    "        #dim_p = int(out_dim * 0.75)\n",
    "        if decoder_prompt: \n",
    "            dim_p = out_dim\n",
    "            self.refiner = FrqRefinerEnhanced(dim = dim_p,\n",
    "                                              h = input_size[0],\n",
    "                                              w = input_size[0])\n",
    "        \n",
    "            self.fused = FrequencyPromptFusionEnhanced(dim = dim_p,\n",
    "                                                       dim_bak= dim,\n",
    "                                                       win_size= 8,\n",
    "                                                       num_heads= 2)\n",
    "        \n",
    "            self.mlp = nn.Conv2d(int(dim_p),int(dim_p),kernel_size=3,bias=False, stride=1, padding=1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(num_features = out_dim)\n",
    "        self.layer_lka_2 = LKABlock(dim=out_dim)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features = out_dim)\n",
    "        def init_weights(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.LayerNorm):\n",
    "                    nn.init.ones_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.Conv2d):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "\n",
    "        init_weights(self)\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        if x2 is not None:  # skip connection exist\n",
    "            x2 = x2.contiguous()\n",
    "            # b, c, h, w = x1.shape\n",
    "            b2, h2, w2, c2 = x2.shape  # e.g: 1 28 28 320, 1 56 56 128\n",
    "            x2 = x2.view(b2, -1, c2)  # e.g: 1 784 320, 1 3136 128\n",
    "\n",
    "            x1_expand = self.x1_linear(x1)  # e.g: 1 784 256 --> 1 784 320, 1 3136 160 --> 1 3136 128\n",
    "\n",
    "            x2_new = x2.view(x2.size(0), x2.size(2), x2.size(1) // w2, x2.size(1) // h2) # B, C, H, W\n",
    "            \n",
    "\n",
    "            x1_expand = x1_expand.view(x2.size(0), x2.size(2), x2.size(1) // w2, x2.size(1) // h2) # B, C, H, W\n",
    "\n",
    "            # print(f'the x1_expand shape is: {x1_expand.shape}\\n\\t the x2_new shape is: {x2_new.shape}')\n",
    "\n",
    "            cat_linear_x = x1_expand + x2_new  # B C H W\n",
    "            cat_linear_x = cat_linear_x.permute(0, 2, 3, 1)  # B H W C\n",
    "            cat_linear_x = self.ag_attn_norm(cat_linear_x)  # B H W C\n",
    "\n",
    "            cat_linear_x = cat_linear_x.permute(0, 3, 1, 2).contiguous()  # B C H W\n",
    "\n",
    "            refined_feature = self.layer_lka_1(cat_linear_x)\n",
    "            \n",
    "            \n",
    "            if self.decoder_prompt:\n",
    "                prompt_layer_1 = self.refiner(refined_feature)\n",
    "                # cat_input_prompt = torch.cat([refined_feature, prompt_layer_1], dim= 1)\n",
    "                fused_map = self.fused(refined_feature, prompt_layer_1)\n",
    "                refined_feature = self.mlp(fused_map).contiguous()\n",
    "\n",
    "            tran_layer_2 = self.bn2(self.layer_lka_2(self.bn1(refined_feature)))\n",
    "\n",
    "            tran_layer_2 = tran_layer_2.view(tran_layer_2.size(0), tran_layer_2.size(3) * tran_layer_2.size(2),\n",
    "                                             tran_layer_2.size(1))\n",
    "            if self.last_layer:\n",
    "                \n",
    "                out = self.last_layer(\n",
    "                    self.layer_up(tran_layer_2).view(b2, 4 * h2, 4 * w2, -1).permute(0, 3, 1, 2))  # 1 9 224 224\n",
    "            else:\n",
    "                out = self.layer_up(tran_layer_2)  # 1 3136 160\n",
    "        else:\n",
    "            out = self.layer_up(x1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_0 = torch.randn(1, 96, 56, 56).cuda() # skip 3\n",
    "input_1 = torch.randn(1, 192, 28, 28).cuda() # skip 2\n",
    "input_2 = torch.randn(1, 384, 14, 14).cuda() # skip 1\n",
    "input_3 = torch.randn(1, 768, 7, 7).cuda() # X\n",
    "\n",
    "\n",
    "\n",
    "decoder_3 = MyDecoderLayerLKAFreqEnhanced(input_size=(7,7),\n",
    "                               in_out_chan=([768, 768]), decoder_prompt=False).cuda()\n",
    "\n",
    "decoder_2 = MyDecoderLayerLKAFreqEnhanced(input_size=(14,14),\n",
    "                               in_out_chan=([384, 384]),\n",
    "                               decoder_prompt=False\n",
    "                               ).cuda()\n",
    "decoder_1 = MyDecoderLayerLKAFreqEnhanced(input_size=(28,28),\n",
    "                               in_out_chan=([192, 192]),\n",
    "                               decoder_prompt=False).cuda()\n",
    "decoder_0 = MyDecoderLayerLKAFreqEnhanced(input_size=(56, 56), \n",
    "                              in_out_chan=([96, 96]),\n",
    "                              decoder_prompt=True,\n",
    "                              is_last=True).cuda()\n",
    "\n",
    "b, c, _, _ = input_3.shape\n",
    "\n",
    "output_3 = decoder_3(input_3.permute(0, 2, 3, 1).view(b,-1, c))\n",
    "output_2 = decoder_2(output_3, input_2.permute(0, 2 , 3, 1))\n",
    "output_1 = decoder_1(output_2, input_1.permute(0, 2, 3, 1))\n",
    "output_0 = decoder_0(output_1, input_0.permute(0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.361232"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_params_in_millions(decoder_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check V8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecoderLayerLKAPrompt(nn.Module):\n",
    "    def __init__(\n",
    "            self, input_size: tuple, in_out_chan: tuple, n_class=9,\n",
    "            norm_layer=nn.LayerNorm, is_last=False, decoder_prompt = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_dim = in_out_chan[0]\n",
    "        x1_dim = in_out_chan[1]\n",
    "        self.decoder_prompt = decoder_prompt\n",
    "        # prompt_ratio = prompt_ratio\n",
    "        \n",
    "        if not is_last:\n",
    "            self.x1_linear = nn.Linear(x1_dim, out_dim)\n",
    "            #self.ag_attn = MultiScaleGatedAttn(dim=x1_dim)\n",
    "            self.ag_attn_norm = nn.LayerNorm(out_dim)\n",
    "\n",
    "            self.layer_up = PatchExpand(input_resolution=input_size, dim=out_dim, dim_scale=2, norm_layer=norm_layer)\n",
    "            self.last_layer = None\n",
    "        else:\n",
    "            self.x1_linear = nn.Linear(x1_dim, out_dim)\n",
    "            #self.ag_attn = MultiScaleGatedAttn(dim=x1_dim)\n",
    "            self.ag_attn_norm = nn.LayerNorm(out_dim)\n",
    "\n",
    "            self.layer_up = FinalPatchExpand_X4(\n",
    "                input_resolution=input_size, dim=out_dim, dim_scale=4, norm_layer=norm_layer\n",
    "            )\n",
    "            self.last_layer = nn.Conv2d(out_dim, n_class, 1)\n",
    "\n",
    "        \n",
    "        self.layer_lka_1 = LKABlock(dim=out_dim)\n",
    "        ## Prompt Module must be located here.\n",
    "\n",
    "        #dim_p = int(out_dim * 0.75)\n",
    "        if decoder_prompt: \n",
    "            dim_p = out_dim\n",
    "            self.prompt1 = LightWeightPromptGenBlock(prompt_dim=dim_p,\n",
    "                                                 input_size= input_size[0],\n",
    "                                                 prompt_len = 5,\n",
    "                                                 lin_dim= dim_p)\n",
    "        \n",
    "            self.noise_level1 = TransformerBlock(dim=int(dim_p*2**1) ,\n",
    "                                             num_heads=1, \n",
    "                                             ffn_expansion_factor=2.66, \n",
    "                                             bias=False, LayerNorm_type='WithBias')\n",
    "        \n",
    "            self.reduce_noise_level1 = nn.Conv2d(int(dim_p*2),int(dim_p*1),kernel_size=1,bias=False)\n",
    "\n",
    "        self.layer_lka_2 = LKABlock(dim=out_dim)\n",
    "\n",
    "        def init_weights(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.LayerNorm):\n",
    "                    nn.init.ones_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.Conv2d):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "\n",
    "        init_weights(self)\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        if x2 is not None:  # skip connection exist\n",
    "            x2 = x2.contiguous()\n",
    "            # b, c, h, w = x1.shape\n",
    "            b2, h2, w2, c2 = x2.shape  # e.g: 1 28 28 320, 1 56 56 128\n",
    "            x2 = x2.view(b2, -1, c2)  # e.g: 1 784 320, 1 3136 128\n",
    "\n",
    "            x1_expand = self.x1_linear(x1)  # e.g: 1 784 256 --> 1 784 320, 1 3136 160 --> 1 3136 128\n",
    "\n",
    "            x2_new = x2.view(x2.size(0), x2.size(2), x2.size(1) // w2, x2.size(1) // h2) # B, C, H, W\n",
    "            \n",
    "\n",
    "            x1_expand = x1_expand.view(x2.size(0), x2.size(2), x2.size(1) // w2, x2.size(1) // h2) # B, C, H, W\n",
    "\n",
    "            # print(f'the x1_expand shape is: {x1_expand.shape}\\n\\t the x2_new shape is: {x2_new.shape}')\n",
    "\n",
    "            cat_linear_x = x1_expand + x2_new  # B C H W\n",
    "            cat_linear_x = cat_linear_x.permute(0, 2, 3, 1)  # B H W C\n",
    "            cat_linear_x = self.ag_attn_norm(cat_linear_x)  # B H W C\n",
    "\n",
    "            cat_linear_x = cat_linear_x.permute(0, 3, 1, 2).contiguous()  # B C H W\n",
    "\n",
    "            refined_feature = self.layer_lka_1(cat_linear_x)\n",
    "            \n",
    "            \n",
    "            if self.decoder_prompt:\n",
    "                prompt_layer_1 = self.prompt1(refined_feature)\n",
    "                cat_input_prompt = torch.cat([refined_feature, prompt_layer_1], dim= 1)\n",
    "                cat_input_prompt = self.noise_level1(cat_input_prompt)\n",
    "                refined_feature = self.reduce_noise_level1(cat_input_prompt)\n",
    "\n",
    "            tran_layer_2 = self.layer_lka_2(refined_feature)\n",
    "\n",
    "            tran_layer_2 = tran_layer_2.view(tran_layer_2.size(0), tran_layer_2.size(3) * tran_layer_2.size(2),\n",
    "                                             tran_layer_2.size(1))\n",
    "            if self.last_layer:\n",
    "                \n",
    "                out = self.last_layer(\n",
    "                    self.layer_up(tran_layer_2).view(b2, 4 * h2, 4 * w2, -1).permute(0, 3, 1, 2))  # 1 9 224 224\n",
    "            else:\n",
    "                out = self.layer_up(tran_layer_2)  # 1 3136 160\n",
    "        else:\n",
    "            out = self.layer_up(x1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecoderLayerLKAFreqEnhancedCat(nn.Module):\n",
    "    def __init__(\n",
    "            self, input_size: tuple, in_out_chan: tuple, n_class=9,\n",
    "            norm_layer=nn.LayerNorm, is_last=False, decoder_prompt = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_dim = in_out_chan[0]\n",
    "        x1_dim = in_out_chan[1]\n",
    "        self.decoder_prompt = decoder_prompt\n",
    "        # prompt_ratio = prompt_ratio\n",
    "        \n",
    "        if not is_last:\n",
    "            self.x1_linear = nn.Linear(x1_dim, out_dim)\n",
    "            #self.ag_attn = MultiScaleGatedAttn(dim=x1_dim)\n",
    "            self.ag_attn_norm = nn.LayerNorm(out_dim)\n",
    "\n",
    "            self.layer_up = PatchExpand(input_resolution=input_size, dim=out_dim, dim_scale=2, norm_layer=norm_layer)\n",
    "            self.last_layer = None\n",
    "        else:\n",
    "            self.x1_linear = nn.Linear(x1_dim, out_dim)\n",
    "            #self.ag_attn = MultiScaleGatedAttn(dim=x1_dim)\n",
    "            self.ag_attn_norm = nn.LayerNorm(out_dim)\n",
    "\n",
    "            self.layer_up = FinalPatchExpand_X4(\n",
    "                input_resolution=input_size, dim=out_dim, dim_scale=4, norm_layer=norm_layer\n",
    "            )\n",
    "            self.last_layer = nn.Conv2d(out_dim, n_class, 1)\n",
    "\n",
    "        \n",
    "        self.layer_lka_1 = LKABlock(dim=out_dim)\n",
    "        ## Prompt Module must be located here.\n",
    "\n",
    "        #dim_p = int(out_dim * 0.75)\n",
    "        if decoder_prompt: \n",
    "            dim_p = out_dim\n",
    "            self.refiner = FrqRefinerEnhanced(dim = dim_p,\n",
    "                                              h = input_size[0],\n",
    "                                              w = input_size[0])\n",
    "        \n",
    "            # self.fused = FrequencyPromptFusionEnhanced(dim = dim_p,\n",
    "            #                                            dim_bak= dim,\n",
    "            #                                            win_size= 8,\n",
    "            #                                            num_heads= 2)\n",
    "            self.noise_level1 = TransformerBlock(dim=int(dim_p*2**1) ,\n",
    "                                             num_heads=1, \n",
    "                                             ffn_expansion_factor=2.66, \n",
    "                                             bias=False, LayerNorm_type='WithBias')\n",
    "            \n",
    "            self.mlp = nn.Conv2d(int(dim_p),int(dim_p),kernel_size=1,bias=False, stride=1, padding=1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(num_features = out_dim)\n",
    "        self.layer_lka_2 = LKABlock(dim=out_dim)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features = out_dim)\n",
    "        def init_weights(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.LayerNorm):\n",
    "                    nn.init.ones_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.Conv2d):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "\n",
    "        init_weights(self)\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        if x2 is not None:  # skip connection exist\n",
    "            x2 = x2.contiguous()\n",
    "            # b, c, h, w = x1.shape\n",
    "            b2, h2, w2, c2 = x2.shape  # e.g: 1 28 28 320, 1 56 56 128\n",
    "            x2 = x2.view(b2, -1, c2)  # e.g: 1 784 320, 1 3136 128\n",
    "\n",
    "            x1_expand = self.x1_linear(x1)  # e.g: 1 784 256 --> 1 784 320, 1 3136 160 --> 1 3136 128\n",
    "\n",
    "            x2_new = x2.view(x2.size(0), x2.size(2), x2.size(1) // w2, x2.size(1) // h2) # B, C, H, W\n",
    "            \n",
    "\n",
    "            x1_expand = x1_expand.view(x2.size(0), x2.size(2), x2.size(1) // w2, x2.size(1) // h2) # B, C, H, W\n",
    "\n",
    "            # print(f'the x1_expand shape is: {x1_expand.shape}\\n\\t the x2_new shape is: {x2_new.shape}')\n",
    "\n",
    "            cat_linear_x = x1_expand + x2_new  # B C H W\n",
    "            cat_linear_x = cat_linear_x.permute(0, 2, 3, 1)  # B H W C\n",
    "            cat_linear_x = self.ag_attn_norm(cat_linear_x)  # B H W C\n",
    "\n",
    "            cat_linear_x = cat_linear_x.permute(0, 3, 1, 2).contiguous()  # B C H W\n",
    "\n",
    "            refined_feature = self.layer_lka_1(cat_linear_x)\n",
    "            \n",
    "            \n",
    "            if self.decoder_prompt:\n",
    "                prompt_layer_1 = self.refiner(refined_feature)\n",
    "                cat_input_prompt = torch.cat([refined_feature, prompt_layer_1], dim= 1)\n",
    "                # fused_map = self.fused(refined_feature, prompt_layer_1)\n",
    "                fused_map = self.noise_level1(cat_input_prompt)\n",
    "                refined_feature = self.mlp(fused_map).contiguous()\n",
    "\n",
    "            tran_layer_2 = self.bn2(self.layer_lka_2(self.bn1(refined_feature)))\n",
    "\n",
    "            tran_layer_2 = tran_layer_2.view(tran_layer_2.size(0), tran_layer_2.size(3) * tran_layer_2.size(2),\n",
    "                                             tran_layer_2.size(1))\n",
    "            if self.last_layer:\n",
    "                \n",
    "                out = self.last_layer(\n",
    "                    self.layer_up(tran_layer_2).view(b2, 4 * h2, 4 * w2, -1).permute(0, 3, 1, 2))  # 1 9 224 224\n",
    "            else:\n",
    "                out = self.layer_up(tran_layer_2)  # 1 3136 160\n",
    "        else:\n",
    "            out = self.layer_up(x1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL1402",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
